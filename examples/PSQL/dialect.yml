# Copyright 2022 Esri
#
# Licensed under the Apache License Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# 
# id of the connector type (mandatory)
id: sample.psql

super class: esri.base
# Insights provides

# configures how SQL identifiers will be generated, mandatory if super class is not defined.
# -----------------
# Descriptions:
# escape character: used to escape special characters. Some common escape characters include \ ' "
# open and close enquotes: used to quote identifiers, such as names for tables/columns/keys/index
# maximum identifier length: maximum number of characters allowed for column and table names
identifier setting:
  escape character: "'"
  open enquote: '"'
  close enquote: '"'
  maximum identifier length: 64
  table prefix: arcgis_insights_
  index prefix: arcgis_insights_index_

# configures some important SQL clauses used in Insights, mandatory if super class is not defined
# Each clause consists of your SQL keyword and placeholders. Insights will replace the placeholders with corresponding contents,
# so make sure the placeholder names are identical to the ones in each clause below.
# Consult WIKI page https://github.com/ArcGIS/insights-connector-guide/wiki/Configure-SQL-Clauses for more details on how to configure this section
sql clauses:
  select constant: SELECT ${columns}
  limit: SELECT * FROM (${source}) t LIMIT ${limit}
  pagination: SELECT * FROM (${source}) t ORDER BY ${orderby} LIMIT ${limit} OFFSET ${offset}
  alter table: ALTER TABLE ${name}
  add constraint: ADD CONSTRAINT
  create table: CREATE TABLE ${name} (${columns} ${primaryKeys})
  create table as: CREATE TABLE ${name} AS ${sql}
  drop table: DROP TABLE ${name}
  insert into: INSERT INTO ${name} (${columns}) VALUES ${values}
  find geometry type: > # When dimension is 0, check if it is multipoint
    SELECT CASE WHEN ST_DIMENSION(${shape}::geometry) = 0
      THEN CASE WHEN ST_NPOINTS(${shape}::geometry) = 1 THEN 0 ELSE 3 END
      ELSE ST_DIMENSION(${shape}::geometry) END as dimension
    FROM ${source}
    WHERE ${shape} IS NOT NULL
    LIMIT 1
  find extent: >
    SELECT xmin, xmax, ymin, ymax
    FROM (SELECT ST_XMin(extent) xmin, ST_YMin(extent) ymin, ST_XMax(extent) xmax, ST_YMax(extent) ymax
      FROM (SELECT ST_Extent(${shape}::geometry) AS extent FROM (${source}) p) q) s
  get spatial reference: >
    SELECT ST_SRID(${shape}) FROM ${source} WHERE ${shape} IS NOT NULL LIMIT 1
  column asc in order by: ${column} ASC nulls first
  column desc in order by: ${column} DESC nulls last
  create attribute index: CREATE INDEX ${index} ON ${table} (${columns})
  create spatial index: CREATE INDEX ${index} ON ${table} USING GIST (${shape})
  
  boxplot: >
    SELECT 1 as category,
           min(base.q1) as q1,
           min(base.median) as median,
           min(base.q3) as q3,
           min(CASE WHEN s.${valueField} < minimumWhisker THEN s.${valueField} ELSE NULL END) as "lowMin",
           max(CASE WHEN s.${valueField} < minimumWhisker THEN s.${valueField} ELSE NULL END) as "lowMax",
           count(CASE WHEN s.${valueField} < minimumWhisker THEN s.${valueField} ELSE NULL END) as "lowCount",
           min(CASE WHEN s.${valueField} > maximumWhisker THEN s.${valueField} ELSE NULL END) as "highMin",
           max(CASE WHEN s.${valueField} > maximumWhisker THEN s.${valueField} ELSE NULL END) as "highMax",
           count(CASE WHEN s.${valueField} >= maximumWhisker THEN s.${valueField} ELSE NULL END) as "highCount",
           max(CASE WHEN s.${valueField} <= maximumWhisker and s.${valueField} >= base.q3 THEN s.${valueField} ELSE NULL END) as "maximum",
           min(CASE WHEN s.${valueField} >= minimumWhisker and s.${valueField} <= base.q1 THEN s.${valueField} ELSE NULL END) as "minimum"
    FROM (${source}) s
    CROSS JOIN
    (
        SELECT minimum,
               case when total < 5 then null else q1 end as q1,
               case when total < 5 then null else median end as median,
               case when total < 5 then null else q3 end as q3,
               maximum,
               case when total < 5 then null else q3+(q3-q1) * 1.5 end  as maximumWhisker,
               case when total < 5 then null else q1-(q3-q1)*1.5 end as minimumWhisker
        FROM (
            SELECT
                    count(${valueField}) as total,
                    min(${valueField}) as minimum,
                    percentile_cont(0.25) WITHIN GROUP (ORDER BY ${valueField}) as q1,
                    percentile_cont(0.5) WITHIN GROUP (ORDER BY ${valueField}) as median,
                    percentile_cont(0.75) WITHIN GROUP (ORDER BY ${valueField}) as q3,
                    max(${valueField}) as maximum
            FROM (${source}) b
        ) b
    ) base

  boxplot with category: >
        SELECT s.${categoryField} as category,
               min(base.q1) as q1,
               min(base.median) as median,
        	   min(base.q3) as q3,
               min(CASE WHEN s.${valueField} < minimumWhisker THEN s.${valueField} ELSE NULL END) as "lowMin",
        	   max(CASE WHEN s.${valueField} < minimumWhisker THEN s.${valueField} ELSE NULL END) as "lowMax",
        	   count(CASE WHEN s.${valueField} < minimumWhisker THEN s.${valueField} ELSE NULL END) as "lowCount",
        	   min(CASE WHEN s.${valueField} > maximumWhisker THEN s.${valueField} ELSE NULL END) as "highMin",
        	   max(CASE WHEN s.${valueField} > maximumWhisker THEN s.${valueField} ELSE NULL END) as "highMax",
               count(CASE WHEN s.${valueField} >= maximumWhisker THEN s.${valueField} ELSE NULL END) as "highCount",
        	   max(CASE WHEN s.${valueField} <= maximumWhisker and s.${valueField} >= base.q3 THEN s.${valueField} ELSE NULL END) as "maximum",
        	   min(CASE WHEN s.${valueField} >= minimumWhisker and s.${valueField} <= base.q1 THEN s.${valueField} ELSE NULL END) as "minimum"
        FROM (${source}) s
        INNER JOIN
        (
        	SELECT
                   ${categoryField},
                   case when total < 5 then null else q1 end as q1,
                   case when total < 5 then null else median end as median,
                   case when total < 5 then null else q3 end as q3,
                   case when total < 5 then null else q3+(q3-q1) * 1.5 end  as maximumWhisker,
                   case when total < 5 then null else q1-(q3-q1)*1.5 end as minimumWhisker
        	FROM (
        		SELECT
                        ${categoryField},
                        count(${valueField}) as total,
        				percentile_cont(0.25) WITHIN GROUP (ORDER BY ${valueField}) as q1,
        				percentile_cont(0.5) WITHIN GROUP (ORDER BY ${valueField}) as median,
        				percentile_cont(0.75) WITHIN GROUP (ORDER BY ${valueField}) as q3
        		FROM (${source}) b
                group by ${categoryField}
        	) b
        ) base on (s.${categoryField} = base.${categoryField}) or
                  (s.${categoryField} is null and base.${categoryField} is null)
        where s.${valueField} is not null
        group by s.${categoryField}
        
# Configures the capabilities of the database. Default value is false.
# "Window functions" will affect what analysis is supported in Insights, while the others decide how Insights create SQL queries.
# -----------------------
# Descriptions:
# window functions: If false, functionalities including but not limited to bar chart, OLS, trendldine will not be supported in Insights. 
# window function in where clause:
#       true if your database supports syntax like: SELECT 1 WHERE id < AVG(num + id)
# named parameters in common table expression:
#       true if your database supports syntax like: WITH some_cte (col1, col2) AS ...
#       false if your datdabase only supports: WITH some_cte AS ...

sql capabilities:
  window functions: true
  window function in where clause: false
  common table expression: true
  named parameters in common table expression: true
  floating point numbers in partition by: true
  support primary key constraint: true
  single row insert: true
  spatial support: true
  write back: true
  requires schema in sql: true
  support alphanumeric indexes: true
  support spatial indexes: true
  support join operation between numbers and strings: true

# configures the sql syntax for window functions used in Insights, mandatory if super class is not defined
# SQL consists of keywords and one or more numbered placeholders
window functions:
  # row number without partition
  # $0: comma separated order by names
  - name: row number
    sql: ROW_NUMBER() OVER(ORDER BY $0)

  # row number with partition
  # $0: comman separated names, $1: comma separated names
  - name: partitioned row number
    sql: ROW_NUMBER() OVER(PARTITION BY $0 ORDER BY $1)

  # average with partition
  # $0: name of the column to be averaged, $1: comma separated column names
  - name: average over
    sql: AVG($0) OVER()

  - name: partitioned average over
    sql: AVG($0) OVER(PARTITION BY $1)

  - name: count over
    sql: COUNT($0) OVER()

  # sum with partition
  - name: partitioned sum over
    sql: SUM($0) OVER(PARTITION BY $1)

  - name: sum over
    sql: SUM($0) OVER()

  # lag functon with offset 1
  - name: lag over
    sql: LAG($0) OVER(ORDER BY $1)

  # sliding average, or moving average without partition
  # $0: name of the column to be averaged, $1: comma separate names, $2 and $3: number that define the width of windows
  - name: sliding average
    sql: AVG($0) OVER(ORDER BY $1 ROWS BETWEEN $2 PRECEDING AND $3 following)

  # sliding average with partition
  # $0: name of the column to be averaged, $1 and $2: comma separate names, $3 and $4: number that define the width of windows
  - name: partitioned sliding average
    sql: AVG($0) OVER(PARTITION BY $1 ORDER BY $2 ROWS BETWEEN $3 PRECEDING AND $4 following)

sql functions:
  # aggregate functions group
  - name: count
    sql: COUNT($0)

  - name: count distinct
    sql: COUNT(DISTINCT $0)

  - name: sum
    sql: SUM($0)

  - name: avg
    sql: AVG($0)

  - name: max
    sql: MAX($0)

  - name: min
    sql: MIN($0)

  - name: stddev
    sql: STDDEV_SAMP($0::DOUBLE PRECISION)

  - name: variance
    sql: VAR_SAMP($0)

  - name: median
    sql: PERCENTILE_CONT(0.5) WITHIN GROUP(ORDER BY $0)

  - name: mode
    sql: MODE() WITHIN GROUP(ORDER BY $0)

  - name: percentile
    sql: PERCENTILE_CONT($0) WITHIN GROUP (ORDER BY $1)
  # Description: Calculates a percentile based on a continuous distribution of the column value in SQL Server. The result is interpolated and might not be equal to any of the specific values in the column.

  # string functions group
  - name: trim
    sql: TRIM(BOTH $0 FROM $1)

  - name: left trim
    sql: TRIM(LEADING $0 FROM $1)

  - name: right trim
    sql: TRIM(TRAILING $0 FROM $1)

  - name: lower
    sql: LOWER($0::text)

  - name: upper
    sql: UPPER($0::text)

  - name: string concatenation
    sql: CONCAT($0, $1)

  - name: string position
    sql: >
      CASE WHEN strpos($1, $0) = 0 THEN NULL ELSE
        strpos($1, $0)
      END

  - name: string position starting with initial position
    sql: >
      CASE WHEN strpos(substr($1, ($2)::int), $0) = 0 THEN NULL ELSE
        strpos(substr($1, ($2)::int), $0) + ($2::int - 1)
      END

  - name: string length
    sql: LENGTH($0::text)

  - name: string substitute
    sql: REPLACE($0, $1, $2)

  - name: substring
    sql: SUBSTRING($0, $1::int, $2::int)

  - name: left
    sql: LEFT($0::text, $1::int)

  - name: right
    sql: RIGHT($0::text, $1::int)

  - name: like
    sql: $0 LIKE $1

  - name: regexp replace
    sql: regexp_replace($0, $1, $2)

  - name: regexp like
    sql: $0 ~ $1

  # date time functions group
  - name: extract year
    sql: DATE_PART('year', $0)
  # Description:  returns a year part of a date
  - name: extract quarter
    sql: DATE_PART('quarter', $0)
  # Description:  returns a quarter part of a date

  - name: extract month
    sql: DATE_PART('month', $0)
  # Description:  returns a month part of a date

  - name: extract day of month
    sql: DATE_PART('day', $0)
  # Description:  returns day of the month part of a date

  - name: extract day of week
    sql: DATE_PART('dow', $0)
  # Description:  returns day of the week part of a date

  - name: extract hour
    sql: DATE_PART('hour', $0)
  # Description:  returns the hour of the month part of a date time field

  - name: extract minute
    sql: DATE_PART('minute', $0)
  # Description:  returns the hour of the month part of a date time field

  - name: extract second
    sql: DATE_PART('second', $0)
  # Description:  returns the hour of the month part of a date time field

  - name: datedif year
    sql: round((date_part('epoch',($1)::date) - date_part('epoch', ($0)::date))::numeric/60/60/24/365)

  # Description: DATEDIFF() function returns the difference between two date time fields in years.

  - name: datedif month
    sql: trunc(date_part('year',age(($1)::date, ($0)::date))*12 + date_part('months',age(($1)::date, ($0)::date)))::numeric
  #DATEDIFF(month, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in months.

  - name: datedif day
    sql: trunc((date_part('epoch', $1) - date_part('epoch', $0))::numeric/60/60/24)
  #DATEDIFF(day, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in days.

  - name: datedif hour
    sql: trunc((date_part('epoch', $1) - date_part('epoch', $0))::numeric/60/60)
  #DATEDIFF(hour, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in hours.

  - name: datedif minute
    sql: trunc((date_part('epoch', $1) - date_part('epoch', $0))::numeric/60)
  #DATEDIFF(minute, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in minutes.

  - name: datedif second
    sql: (trunc(date_part('epoch', $1) - date_part('epoch', $0))::NUMERIC)
  #DATEDIFF(second, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in seconds.

  - name: timedif hour
    sql: DATE_PART('hour', $1::time - $0::time)
  #DATEDIFF(hour, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in hours.

  - name: timedif minute
    sql: DATE_PART('hour', $1::time - $0::time) * 60 + DATE_PART('minute', $1::time - $0::time)
  #DATEDIFF(minute, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in minutes.

  - name: timedif second
    sql:  (DATE_PART('hour', $1::time - $0::time) * 60 + DATE_PART('minute', $1::time - $0::time)) * 60 + DATE_PART('second', $1::time - $0::time)
  #DATEDIFF(second, $0, $1)
  # Description: DATEDIFF() function returns the difference between two date time fields in seconds.

  - name: dateadd year
    sql: $0 + ($1* interval '1 year')

  - name: dateadd month
    sql: $0 + ($1* interval '1 month')

  - name: dateadd day
    sql: $0 + ($1* interval '1 day')

  - name: dateadd hour
    sql: $0 + ($1* interval  '1 hour')

  - name: dateadd minute
    sql: $0 + ($1* interval '1 minute')

  - name: dateadd second
    sql: $0 + ($1* interval '1 second')

  - name: timeonlyadd hour
    sql: $0 + ($1* interval '1 hour')

  - name: timeonlyadd minute
    sql: $0 + ($1* interval '1 minute')

  - name: timeonlyadd second
    sql: $0 + ($1* interval '1 second')

  - name: create timestamp from milliseconds
    sql: (to_timestamp($0::DOUBLE PRECISION / 1000) AT TIME ZONE 'UTC')

  - name: create timestamp from components
    sql: >
      CASE WHEN $1 > 12 OR $1 < 1 OR $2 > 31 OR $2 < 1 OR
        $3 > 24 OR $3 < 0 OR $4 > 60 OR $4 < 0 OR $5 > 60 OR $5 < 0 THEN NULL ELSE
        TO_TIMESTAMP($0 || '-' || $1 || '-' || $2 || ' ' || RIGHT('0' || $3, 2) || ':' || $4 || ':' || $5,
              CASE WHEN length($0::text) > 2 THEN 'YYYY-MM-DD hh24:mi:ss' ELSE 'YY-MM-DD hh24:mi:ss' END)::timestamp without time zone
      END
  # Description: Creates a timestamp from string 2 possible string timestamp format masks 'YYYY-MM-DD hh24:mi:ss' and 'YY-MM-DD hh24:mi:ss'
  # Syntax
  # $0 Year part of the date component
  # $1 Month part of the date component
  # $2 Day part of the date component
  # $3 Hour part of the time component
  # $4 Minutes part of the time component
  # $5 Seconds part of the time component

  - name: create timestamp from text
    sql: TO_TIMESTAMP($0, '$1')::timestamp without time zone
  # Description: Creates a time from string field using a specified fromat mask
  # Syntax:
  # $0 string to convert to time
  # $1 format string

  - name: create time from milliseconds
    sql: (TO_TIMESTAMP($0/1000.0)  AT TIME ZONE 'UTC') ::time
  # Description: Creates a time value from the number of milliseconds
  # Syntax:
  # $0 Milliseconds to convert to time

  - name: create time from components
    sql: >
      CASE WHEN $0 > 24 OR $0 < 0 OR $1 > 60 OR $1 < 0 OR $2 > 60 OR $2 < 0 THEN NULL ELSE
      ($0 || ':' || $1 || ':' || $2)::time END
  # Description: Creates a time value from a string time format masks  'hh24:mi:ss'
  # Syntax
  # $0 Hour part of the time component
  # $1 Minutes part of the time component
  # $2 Seconds part of the time component

  - name: create date from milliseconds
    sql: (TO_TIMESTAMP($0::DOUBLE PRECISION/1000.0) AT TIME ZONE 'UTC')

  - name: create date from components
    sql: >
      CASE WHEN $1 > 12 OR $1 < 1 OR $2 > 31 OR $2 < 1 THEN NULL ELSE
      TO_DATE($0 || '-' || $1 || '-' ||$2,
              CASE WHEN length($0::text) > 2 THEN 'YYYY-MM-DD' ELSE 'YY-MM-DD' END) END
  # Description: Creates a date from string 2 possible string date format masks  'YYYY-MM-DD' and 'YY-MM-DD'
  # Syntax
  # $0 Year part of the date component
  # $1 Month part of the date component
  # $2 Day part of the date component

  - name: generate time series
    sql: Generate_series($0, $1, '$2 $3')

  - name: time from hours minutes seconds
    sql: >
      CASE WHEN $0 > 24 OR $0 < 0 OR $1 > 60 OR $1 < 0 OR $2 > 60 OR $2 < 0 THEN NULL ELSE
      ($0 || ':' || $1 || ':' || $2)::time END

  - name: current timestamp
    sql: CURRENT_TIMESTAMP
  # Description: returns the current timestamp.

  - name: current time
    sql: CURRENT_DATE
  # Description: returns the current time.

  - name: current date
    sql: CURRENT_DATE
  # Description: returns the current date.

  - name: now
    sql: NOW()

  # conditional functions group

  - name: case when else
    sql: CASE WHEN $0 THEN $1 ELSE $2 END
  # Description: The CASE statement goes through conditions and returns a value when the first condition is met. So, once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the ELSE clause.

  - name: case when
    sql: CASE WHEN $0 THEN $1 END
  # Description: The CASE statement goes through conditions and returns a value when the first condition is met. So, once a condition is true, it will stop reading and return the result.

  - name: is null
    sql: $0 IS NULL

  - name: is not null
    sql: $0 IS NOT NULL

  - name: nullif
    sql: nullif($0, $1)

  - name: ifnull
    sql: COALESCE($0, $1)

  - name: coalesce
    sql: COALESCE($0, $1)

  # logical operators
  - name: and
    sql: $0 AND $1

  - name: or
    sql: $0 OR $1

  - name: between
    sql: $0 BETWEEN $1 AND $2
  # Description: Operator selects values within a given range. The values can be numbers, text, or dates.
  # Syntax:
  # $0 column being filtered
  # $1 lower value of the range 
  # $2 Upper value of the range

  - name: not
    sql: NOT($0)

  # arithmetic functions group
  - name: abs
    sql: ABS($0)

  - name: acos
    sql: ACOS($0)

  - name: asin
    sql: ASIN($0)

  - name: atan
    sql: ATAN($0)

  - name: sin
    sql: SIN($0)

  - name: sinh
    sql: SINH($0)

  - name: cos
    sql: COS($0)

  - name: cosh
    sql: COSH($0)

  - name: tan
    sql: TAN($0)

  - name: tanh
    sql: TANH($0)

  - name: ln
    sql: LN(NULLIF(CASE WHEN $0 <= 0 THEN NULL ELSE $0 END ,0))
  # Description: Returns the natural logarithm of a number.

  - name: log
    sql: LOG(CASE WHEN $1 <= 1 THEN NULL ELSE ($1)::numeric END, nullif(CASE WHEN $0 <= 0 THEN 0 ELSE ($0)::numeric END, 0))
  # Description: Returns the natural logarithm of a specified number, or the logarithm of the number to the specified base

  - name: log10
    sql: LOG(CASE WHEN $0 <= 0 THEN NULL ELSE $0 END)
  # Description: Returns the natural logarithm of a number to base-10.

  - name: negation
    sql: -($0)

  - name: add
    sql: ($0::numeric) + ($1)

  - name: subtract
    sql: ($0::numeric) - ($1)

  - name: multiply
    sql: $0::numeric * $1

  - name: divide
    sql: $0::numeric / NULLIF($1,0)

  - name: power
    sql: power($0, CASE WHEN $0 < 0 AND ceil(abs(mod(($1)::numeric ,1))) != 0  THEN NULL ELSE $1 END)
  # Description: Returns the value of a number raised to the power of another number.

  - name: mod
    sql: MOD($0, $1)
  # Description:  Returns the remainder of a number divided by another number.
  # Syntax:
  # $0 Column to be deivided
  # $1 The divisor

  - name: round
    sql: ROUND($0::numeric)
  # Description: Rounds a decimal to the closest integer.

  - name: round to
    sql: ROUND($0::numeric, CAST($1 AS INT))
  # Description: Rounds a number to a specified number of decimal places.
  # Syntax:
  # $0 Column to be rounded
  # $1 The number of decimal places to round number to. 

  - name: ceiling
    sql: CEILING($0)
  # Description: Returns the smallest integer value that is bigger than or equal to a number.

  - name: floor
    sql: FLOOR($0)
  # Description: Returns the largest integer value that is smaller than or equal to a number.

  - name: sign
    sql: SIGN($0)
  # Description: Returns the sign of a number.
  # This function will return one of the following:
  #  If $0 > 0, it returns 1
  #  If $0 = 0, it returns 0
  #  If $0 < 0, it returns -1

  - name: truncate
    sql: TRUNC($0, $1)
  # Description: This function truncates a number to the specified number of decimal places.  
  # Syntax:  
  # $0 - Column to be truncated (number)
  # $1 - number of decimal places to truncate to

  # conversion functions
  - name: cast
    sql: CAST($0 AS $1)
  # Description: The CAST() function converts a value (of any type) into a specified data type. The supported data type are shown below in "column types" section
  # Syntax: 
  # $0 - Column to be converted
  # $1 - data type to convert column to {see "column types"}

  # spatial functions
  - name: st_x
    sql: ST_X($0::geometry)

  - name: st_y
    sql: ST_Y($0::geometry)

  - name: st_within
    sql: ST_WITHIN($0::geometry, $1::geometry)

  - name: st_asbinary
    sql: ST_ASBINARY($0)

  - name: st_geometryFromText
    sql: ST_GeomFromText($0, $1)

  - name: st_geographyFromText
    sql: ST_GeographyFromText($0)

  - name: st_intersects
    sql: ST_Intersects($0, $1)

  - name: st_geometryFromText
    sql: ST_GeomFromText($0, $1)

  - name: st_geographyFromText
    sql: ST_GeographyFromText($0)

  - name: st_intersects
    sql: ST_Intersects($0, $1)

  - name: st_asbinary
    sql: ST_ASBINARY($0)

  - name: make_point
    sql: ST_SETSRID(ST_POINT($0, $1), $2)

  - name: st_centroid
    sql: ST_Centroid($0::geometry)

  - name: st_distance_sphere
    sql: ST_DistanceSphere($0::geometry, $1::geometry)

  - name: linestring
    sql: ST_SetSrid(ST_MakeLine($0::geometry, $1::geometry), $2)

  - name: geodesic length
    sql: ST_PERIMETER($0::geography)

  - name: geodesic area
    sql: ST_AREA($0::geography)

  - name: st_transform
    sql: st_transform($0::geometry, $1)

  - name: st_buffer
    sql: ST_BUFFER($0::GEOGRAPHY, $1)

# mandatory configuration if super class is not defined


# Configures the types used for casting, mandatory if super class is not defined
# for example if the string type is configured as `VARCHAR(255)`, then when Insights needs to cast strings,
# our SQL will look like `CAST($0 AS VARCHAR(255))`.
# Make sure the string type has enough length, and the float type has enough precision for your data
column types:
  integer: integer
  float: numeric
  fixed: numeric(38, 15)
  string: varchar(255)
  date: date
  timestamp: timestamp without time zone
  time: time
  geometry: geometry
  GEOGRAPHY: GEOGRAPHY
sql placeholders:
  integer: '?'
  float: '?'
  fixed: '?'
  string: '?'
  date: '?'
  timestamp: '?'
  time: '?'
  geometry: '?'

map to field types:

  geometry: geometry
  GEOGRAPHY: GEOGRAPHY

# Configures the sql syntax for date and time formats, mandatory if section "date format codes" is not defined
# Left side is the format you will use in DATEVALUE function, and right side is the format SQL you tell us to use in SQL queries.
# YY: two digit year
# YYYY: four digit year
# MM: month (1-12)
# DD: day of month (1-31)
# hh: hour (0-12)
# hh24: hour (0-24)
# mi: minute
# ss: second 
# ms: millisecond
# mon: month name (January, February etc.)
# ampm: am or pm
# --------------------- 
# Example: Based on the below configuration, for a date string '1999-01-01 10:20:30 am', you must provide us the following format in  
# DATE VALUE calculation: DATEVALUE('1999-01-01 10:20:30 am', 'YYYY-MM-DD hh:mi:ss ampm'). The letter cases must be the same.
# Insights will map the format to your configuration on the right, so the above format will be translated to
# 'yyyy-MM-dd hh:mi:ss am' and used in SQL queries.
date and time sql tokens:
  YY: yy
  YYYY: yyyy
  MM: MM
  DD: dd
  hh: hh
  HH: HH24
  mm: mi
  ss: ss
  ms: ms
  mon: mon
  month: month
  ampm: am

# Configures some shortcut for date and time format mapping (Optional)
# Left side is the format you will use in DATEVALUE function, and right side is the format SQL you tell us to use in SQL queries.
# You may configure this section if you have a set of most frequently used formats. If this section exists, Insights will always try to 
# find an exact match from the list here before trying to assemble one using the `date and time sql tokens` above.
# You have to configure this section if your database does not use specific tokens for date and time formats, like SQL Server.
# --------------------
# Usage: If the format code for 'mm/dd/yyyy' for your database is 101, then you should put "mm/dd/yyyy":101 in this seciton.
#        When you calculate DATEVALUE(date, 'mm/dd/yyyy') in Insights, we will be able to use the 101 code directly.
date format codes:
  # US
  "mm/dd/yy": mm/dd/yy
  "mm/dd/yyyy": mm/dd/yyyy
  # ANSI
  "yy.mm.dd": yy.mm.dd
  "yyyy.mm.dd": yyyy.mm.dd
  # British/French
  "dd/mm/yy": dd/mm/yy
  "dd/mm/yyyy": dd/mm/yyyy
  # German
  "dd.mm.yy": dd.mm.yy
  "dd.mm.yyyy": dd.mm.yyyy
  # Italian
  "dd-mm-yy": dd-mm-yy
  "dd-mm-yyyy": dd-mm-yyyy
  # Japan
  "yy/mm/dd": yy/mm/dd
  "yyyy/mm/dd": yyyy/mm/dd
  # ISO
  "yymmdd": yymmdd
  "yyyymmdd": yyyymmdd
  # month name
  "dd mon yy": dd mon yy
  "dd mon yyyy": dd mon yyyy
  "Mon dd, yy": mon dd, yy
  "Mon dd, yyyy": mon dd, yyyy

# Configures the state code for different types of sql exceptions. This section is optional, but it helps Insights displays correct error message.
# Some databases publish their error codes, for example 
#   Postgres: https://www.postgresql.org/docs/10/errcodes-appendix.html
sql error states:
  invalid datetime format: 22007
  datetime overflow: 22008
  query cancelled: 57014
  connection failed: 08001
  cannot connect now: 57P03
  permission error: 42501